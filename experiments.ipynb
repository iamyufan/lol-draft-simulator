{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "#from collections import Counter\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, games_path: str = None, champion_info_path: str = None, champion_info_path_2: str = None):\n",
    "        if games_path and champion_info_path:\n",
    "            self.games_df = pd.read_csv(games_path)\n",
    "            with open(champion_info_path, \"r\") as f:\n",
    "                self.champion_info = json.load(f)[\"data\"]\n",
    "            \n",
    "            with open(champion_info_path_2, \"r\") as f:\n",
    "                self.champion_info_2 = json.load(f)[\"data\"]\n",
    "            \n",
    "            self.all_tags = sorted({\n",
    "                tag\n",
    "                for info in self.champion_info_2.values()\n",
    "                for tag in info.get(\"tags\", [])\n",
    "            })\n",
    "            # Remove gameDuration < 17min\n",
    "            self.games_df = self.games_df[self.games_df[\"gameDuration\"] >= 1020]\n",
    "            # Remove outliers\n",
    "            # Calculate the IQR for gameDuration\n",
    "            # IQR = Q3 - Q1\n",
    "            durations = self.games_df[\"gameDuration\"]\n",
    "            Q1 = durations.quantile(0.25)\n",
    "            Q3 = durations.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            upper_fence = Q3 + 1.5 * IQR\n",
    "            self.games_df = self.games_df[durations <= upper_fence]\n",
    "\n",
    "        else:\n",
    "            self.games_df = None\n",
    "            self.champion_info = None\n",
    "        self.stats_cols = [\"firstBlood\"]\n",
    "\n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        if games_path and champion_info_path:\n",
    "            # Create mappings\n",
    "            # Create a list of all champion keys from the dictionary\n",
    "            champion_keys = [\n",
    "                champion[\"key\"]\n",
    "                for champion in self.champion_info.values()\n",
    "                if champion[\"key\"] != \"None\"\n",
    "            ]\n",
    "            champion_keys.sort()\n",
    "\n",
    "            print(f\"Number of champions: {len(champion_keys)}\")\n",
    "            print(f\"Number of games: {len(self.games_df)}\")\n",
    "\n",
    "            # Create mappings\n",
    "            self.champion_id_to_key = {\n",
    "                champion[\"id\"]: champion[\"key\"]\n",
    "                for champion in self.champion_info.values()\n",
    "                if champion[\"key\"] != \"None\"\n",
    "            }\n",
    "            self.champion_key_to_id = {\n",
    "                champion[\"key\"]: champion[\"id\"]\n",
    "                for champion in self.champion_info.values()\n",
    "                if champion[\"key\"] != \"None\"\n",
    "            }\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save the data processor with its fitted scaler.\"\"\"\n",
    "        # Create a dictionary of attributes to save\n",
    "        processor_state = {\n",
    "            'champion_info': self.champion_info,\n",
    "            'champion_info_2': self.champion_info_2,\n",
    "            'all_tags': self.all_tags,\n",
    "            'champion_id_to_key': self.champion_id_to_key,\n",
    "            'champion_key_to_id': self.champion_key_to_id,\n",
    "            'scaler': self.scaler,\n",
    "            'games_df': self.games_df\n",
    "        }\n",
    "        joblib.dump(processor_state, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        \"\"\"Load a saved data processor.\"\"\"\n",
    "        processor_state = joblib.load(path)\n",
    "        \n",
    "        # Create a new instance\n",
    "        processor = cls()\n",
    "        \n",
    "        # Restore the state\n",
    "        processor.champion_info = processor_state['champion_info']\n",
    "        processor.champion_info_2 = processor_state['champion_info_2']\n",
    "        processor.all_tags = processor_state['all_tags']\n",
    "        processor.champion_id_to_key = processor_state['champion_id_to_key']\n",
    "        processor.champion_key_to_id = processor_state['champion_key_to_id']\n",
    "        processor.scaler = processor_state['scaler']\n",
    "        processor.games_df = processor_state['games_df']\n",
    "        \n",
    "        return processor\n",
    "\n",
    "    def prepare_train_test_split(self, test_size=0.2, random_state=42):\n",
    "        X, y = self.process_data()\n",
    "\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Fit and transform the training data\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "\n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"Process the data to create features and labels.\n",
    "        \n",
    "        Returns:\n",
    "            X: pd.DataFrame - Features\n",
    "            y: pd.Series - Labels\n",
    "        \"\"\"\n",
    "        # Create features for both teams\n",
    "        team1_features = self._create_team_features(\"t1\")\n",
    "        team2_features = self._create_team_features(\"t2\")\n",
    "        team1_tag_counts = self._create_tag_count_features(\"t1\")\n",
    "        team2_tag_counts = self._create_tag_count_features(\"t2\")\n",
    "        stats_cols = [\n",
    "            \"firstBlood\"\n",
    "        ]\n",
    "        game_stats = self.games_df[stats_cols]\n",
    "        # Combine features\n",
    "        X = pd.concat([team1_features, team2_features, team1_tag_counts, team2_tag_counts, game_stats], axis=1)\n",
    "        y = self.games_df[\"winner\"].map(\n",
    "            {1: 1, 2: 0}\n",
    "        )  # Convert to binary (1 = team1 wins, 0 = team2 wins)\n",
    "        \n",
    "        # Remove All 0 Columns\n",
    "        # variances = X.var(axis=0)\n",
    "        # zero_var = variances[variances == 0].index.tolist()\n",
    "        # if zero_var:\n",
    "        #     print(f\"Dropping zero‐variance columns: {zero_var}\")\n",
    "        #     X = X.drop(columns=zero_var)\n",
    "        # self._dropped_cols = zero_var\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def _create_tag_count_features(self, team_prefix: str):\n",
    "        \"\"\"\n",
    "        count the number of each tag in each team\n",
    "        for_example team1_tank_num, team2_fighter_num。\n",
    "        \"\"\"\n",
    "        cols = [f\"{team_prefix}_{tag.lower()}_num\" for tag in self.all_tags]\n",
    "        df = pd.DataFrame(0, index=self.games_df.index, columns=cols)\n",
    "\n",
    "        for tag in self.all_tags:\n",
    "            col = f\"{team_prefix}_{tag.lower()}_num\"\n",
    "            def count_tag(row):\n",
    "                cnt = 0\n",
    "                for i in range(1, 6):\n",
    "                    champ_id = row[f\"{team_prefix}_champ{i}id\"]\n",
    "                    champ_key = self.champion_id_to_key.get(champ_id)\n",
    "                    info = self.champion_info_2.get(champ_key, {})\n",
    "                    if tag in info.get(\"tags\", []):\n",
    "                        cnt += 1\n",
    "                return cnt\n",
    "            df[col] = self.games_df.apply(count_tag, axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def _create_team_features(self, team_prefix):\n",
    "        \"\"\"Create features for a specific team.\n",
    "        \n",
    "        Args:\n",
    "            team_prefix: str - Prefix for the team (either \"t1\" or \"t2\")\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with features in the format expected by the model\n",
    "        \"\"\"\n",
    "        # Initialize the feature columns\n",
    "        feature_cols = [\n",
    "            f\"{champ_key}_picked_{team_prefix}\"\n",
    "            for champ_key in self.champion_id_to_key.values()\n",
    "        ]\n",
    "        # start all zeros\n",
    "        feature_df = pd.DataFrame(0, index=self.games_df.index, columns=feature_cols)\n",
    "\n",
    "        # for each pick‐slot, ADD into the correct champion column\n",
    "        for i in range(1, 6):\n",
    "            slot_col = f\"{team_prefix}_champ{i}id\"\n",
    "            # vectorized per‐champ:\n",
    "            for champ_id, champ_key in self.champion_id_to_key.items():\n",
    "                mask = (self.games_df[slot_col] == champ_id)\n",
    "                feature_df.loc[mask, f\"{champ_key}_picked_{team_prefix}\"] += 1\n",
    "\n",
    "        return feature_df\n",
    "\n",
    "    def get_champion_name(self, champion_id: int) -> str:\n",
    "        \"\"\"Convert champion ID to name.\"\"\"\n",
    "        return self.champion_id_to_key.get(champion_id, \"Unknown\")\n",
    "\n",
    "    def get_all_champions(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get list of all champions with their IDs and names.\"\"\"\n",
    "        return [\n",
    "            {\"id\": str(id), \"name\": name}\n",
    "            for id, name in self.champion_id_to_key.items()\n",
    "        ]\n",
    "\n",
    "    def prepare_prediction_data(\n",
    "        self,\n",
    "        team1_champs: List[str],\n",
    "        team2_champs: List[str],\n",
    "    ) -> np.ndarray:\n",
    "        # 1) build the same pick+tag columns you used in process_data()\n",
    "        feature_cols = []\n",
    "        for t in (\"t1\", \"t2\"):\n",
    "            # champion‐pick columns, in the same order as your training code\n",
    "            for champ_key in self.champion_key_to_id:\n",
    "                feature_cols.append(f\"{champ_key}_picked_{t}\")\n",
    "            # tag‐count columns\n",
    "            for tag in self.all_tags:\n",
    "                feature_cols.append(f\"{t}_{tag.lower()}_num\")\n",
    "\n",
    "        # 2) initialize a zero‐row DataFrame\n",
    "        features = pd.DataFrame(0, index=[0], columns=feature_cols)\n",
    "\n",
    "        # 3) fill in picks\n",
    "        for champ in team1_champs:\n",
    "            features.at[0, f\"{champ}_picked_t1\"] += 1\n",
    "        for champ in team2_champs:\n",
    "            features.at[0, f\"{champ}_picked_t2\"] += 1\n",
    "\n",
    "        # 4) fill in tag‐counts\n",
    "        for champ in team1_champs:\n",
    "            for tag in self.champion_info_2[champ][\"tags\"]:\n",
    "                features.at[0, f\"t1_{tag.lower()}_num\"] += 1\n",
    "        for champ in team2_champs:\n",
    "            for tag in self.champion_info_2[champ][\"tags\"]:\n",
    "                features.at[0, f\"t2_{tag.lower()}_num\"] += 1\n",
    "\n",
    "        # 5) make sure your stats columns are present (and in the same order!)\n",
    "        #    you probably should have done `self.stats_cols = [...]` in __init__\n",
    "        for stat in self.stats_cols:\n",
    "            if stat not in features.columns:\n",
    "                features[stat] = 0.0\n",
    "\n",
    "        # 6) **THIS IS THE KEY**: reorder to exactly what the scaler saw during fit\n",
    "        features = features.loc[:, self.scaler.feature_names_in_]\n",
    "\n",
    "        # 7) finally scale and return\n",
    "        return self.scaler.transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "\n",
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, C: float = 1.0, lr: float = 0.01, n_iter: int = 1000):\n",
    "        \"\"\"\n",
    "        C     : inverse regularization strength (bigger C => less reg)\n",
    "        lr    : learning rate\n",
    "        n_iter: number of gradient steps\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        n_samples, n_features = X.shape\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "        # lambda = 1/C\n",
    "        lambda_param = 1.0 / self.C\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            # linear model\n",
    "            linear = X.dot(self.weights) + self.bias\n",
    "            # sigmoid\n",
    "            y_pred = 1.0 / (1.0 + np.exp(-linear))\n",
    "\n",
    "            # gradients w/ L2 regularization\n",
    "            dw = (1.0 / n_samples) * X.T.dot(y_pred - y) \\\n",
    "                 + lambda_param * self.weights\n",
    "            db = (1.0 / n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            # update\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias    -= self.lr * db\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        linear = X.dot(self.weights) + self.bias\n",
    "        probs  = 1.0 / (1.0 + np.exp(-linear))\n",
    "        return np.vstack([1-probs, probs]).T\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return (self.predict_proba(X)[:,1] >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "class SVMScratch:\n",
    "    def __init__(self, C=1.0, lr=1e-3, n_iter=1000, kernel='linear', gamma='scale', n_components=100):\n",
    "        self.C = C\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.n_components = n_components\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.W = None\n",
    "        self.basis = None\n",
    "\n",
    "    def _compute_gamma(self, X):\n",
    "        n_features = X.shape[1]\n",
    "        if self.gamma == 'scale':\n",
    "            var = np.var(X, axis=0).mean()\n",
    "            return 1.0 / (n_features * var) if var > 0 else 1.0\n",
    "        elif self.gamma == 'auto':\n",
    "            return 1.0 / n_features\n",
    "        else:\n",
    "            return float(self.gamma)\n",
    "\n",
    "    def _rbf_feature_map(self, X):\n",
    "        proj = X.dot(self.W) + self.basis\n",
    "        return np.sqrt(2.0/self.n_components) * np.cos(proj)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # map {0,1} → {-1,+1}\n",
    "        y_signed = np.where(y<=0, -1, +1)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # 1) transform to RFF if needed\n",
    "        if self.kernel == 'rbf':\n",
    "            γ = self._compute_gamma(X)\n",
    "            self.W     = np.random.normal(scale=np.sqrt(2*γ), size=(n_features, self.n_components))\n",
    "            self.basis = np.random.uniform(0, 2*np.pi, size=self.n_components)\n",
    "            X_train = self._rbf_feature_map(X)\n",
    "        else:\n",
    "            X_train = X\n",
    "\n",
    "        # 2) init\n",
    "        D = X_train.shape[1]\n",
    "        self.w = np.zeros(D)\n",
    "        self.b = 0.0\n",
    "\n",
    "        # 3) SGD\n",
    "        for _ in range(self.n_iter):\n",
    "            for i in range(n_samples):\n",
    "                xi = X_train[i]\n",
    "                yi = y_signed[i]\n",
    "                margin = yi * (xi.dot(self.w) + self.b)\n",
    "\n",
    "                if margin >= 1:\n",
    "                    # only regularizer ∂(½‖w‖²)/∂w = w\n",
    "                    grad_w = self.w\n",
    "                    grad_b = 0.0\n",
    "                else:\n",
    "                    # ∂hinge/∂w = –C yi xi, ∂hinge/∂b = –C yi\n",
    "                    grad_w = self.w - self.C * yi * xi\n",
    "                    grad_b = -self.C * yi\n",
    "\n",
    "                self.w -= self.lr * grad_w\n",
    "                self.b -= self.lr * grad_b\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        if self.kernel == 'rbf':\n",
    "            Z = self._rbf_feature_map(X)\n",
    "            return Z.dot(self.w) + self.b\n",
    "        else:\n",
    "            return X.dot(self.w) + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.decision_function(X) >= 0).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        scores = self.decision_function(X)\n",
    "        probs = 1.0/(1.0 + np.exp(-scores))\n",
    "        return np.vstack([1-probs, probs]).T\n",
    "\n",
    "    \n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value  # for leaf nodes\n",
    "\n",
    "\n",
    "class DecisionTreeScratch:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        num_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if (depth >= self.max_depth\n",
    "            or n_samples < self.min_samples_split\n",
    "            or num_labels == 1):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        best_feat, best_thresh = self._best_criteria(X, y, n_features)\n",
    "        if best_feat is None:\n",
    "            return Node(value=self._most_common_label(y))\n",
    "\n",
    "        left_idx, right_idx = self._split(X[:, best_feat], best_thresh)\n",
    "        left  = self._grow_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
    "        right = self._grow_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "    def _best_criteria(self, X, y, n_features):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feat in range(n_features):\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for t in thresholds:\n",
    "                left_idx, right_idx = self._split(X[:, feat], t)\n",
    "                # **skip** thresholds that put everything on one side\n",
    "                if len(left_idx) == 0 or len(right_idx) == 0:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, X[:, feat], t)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat\n",
    "                    split_thresh = t\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _information_gain(self, y, X_col, split_thresh):\n",
    "        # parent Gini\n",
    "        parent_gini = self._gini(y)\n",
    "        left_idx, right_idx = self._split(X_col, split_thresh)\n",
    "        if len(left_idx) == 0 or len(right_idx) == 0:\n",
    "            return 0\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idx), len(right_idx)\n",
    "        gini_l = self._gini(y[left_idx])\n",
    "        gini_r = self._gini(y[right_idx])\n",
    "        # weighted\n",
    "        child_gini = (n_l/n) * gini_l + (n_r/n) * gini_r\n",
    "        return parent_gini - child_gini\n",
    "\n",
    "    def _gini(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        ps = counts / counts.sum()\n",
    "        return 1 - np.sum(ps**2)\n",
    "\n",
    "    def _split(self, X_col, split_thresh):\n",
    "        left_idx  = np.where(X_col <= split_thresh)[0]\n",
    "        right_idx = np.where(X_col >  split_thresh)[0]\n",
    "        return left_idx, right_idx\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def _traverse(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse(x, node.left)\n",
    "        return self._traverse(x, node.right)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "    \n",
    "class RandomForestScratch:\n",
    "    def __init__(self,\n",
    "                 n_estimators=100,\n",
    "                 max_depth=10,\n",
    "                 min_samples_split=2,\n",
    "                 max_features='sqrt'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.trees: List[DecisionTreeScratch] = []\n",
    "        self.features_idxs: List[np.ndarray] = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        y = np.asarray(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.max_features == 'sqrt':\n",
    "            max_feats = int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            max_feats = int(np.log2(n_features))\n",
    "        else:\n",
    "            max_feats = n_features\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # bootstrap sample\n",
    "            idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_samp, y_samp = X[idxs], y[idxs]\n",
    "            feat_idxs = np.random.choice(n_features, max_feats, replace=False)\n",
    "\n",
    "            tree = DecisionTreeScratch(\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_depth=self.max_depth\n",
    "            )\n",
    "            # train on subset of features\n",
    "            tree.fit(X_samp[:, feat_idxs], y_samp)\n",
    "            self.trees.append(tree)\n",
    "            self.features_idxs.append(feat_idxs)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # collect each tree's predictions\n",
    "        all_preds = np.array([\n",
    "            tree.predict(X[:, fi]) for tree, fi in zip(self.trees, self.features_idxs)\n",
    "        ])\n",
    "        # majority vote\n",
    "        return np.apply_along_axis(lambda row: np.bincount(row).argmax(),\n",
    "                                   axis=0,\n",
    "                                   arr=all_preds)\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        # fraction of trees voting class 1\n",
    "        all_votes = np.array([\n",
    "            tree.predict(X[:, fi]) for tree, fi in zip(self.trees, self.features_idxs)\n",
    "        ])\n",
    "        proba1 = all_votes.mean(axis=0)\n",
    "        return np.vstack([1 - proba1, proba1]).T\n",
    "    \n",
    "class DecisionTreeRegressorScratch:\n",
    "    def __init__(self, min_samples_split=2, max_depth=3):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if (depth >= self.max_depth) or (n_samples < self.min_samples_split):\n",
    "            leaf_val = y.mean()\n",
    "            return Node(value=leaf_val)\n",
    "\n",
    "        best_feat, best_thresh = None, None\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for feat in range(n_features):\n",
    "            for t in np.unique(X[:, feat]):\n",
    "                left_idx, right_idx = np.where(X[:, feat] <= t)[0], np.where(X[:, feat] > t)[0]\n",
    "                if len(left_idx) == 0 or len(right_idx) == 0:\n",
    "                    continue\n",
    "                loss = (\n",
    "                    len(left_idx) * y[left_idx].var() +\n",
    "                    len(right_idx)* y[right_idx].var()\n",
    "                ) / n_samples\n",
    "                if loss < best_loss:\n",
    "                    best_loss, best_feat, best_thresh = loss, feat, t\n",
    "\n",
    "        if best_feat is None:\n",
    "            return Node(value=y.mean())\n",
    "\n",
    "        left_idx, right_idx = np.where(X[:, best_feat] <= best_thresh)[0], np.where(X[:, best_feat] > best_thresh)[0]\n",
    "        left = self._grow_tree(X[left_idx], y[left_idx], depth+1)\n",
    "        right= self._grow_tree(X[right_idx], y[right_idx], depth+1)\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "    def _traverse(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse(x, node.left)\n",
    "        return self._traverse(x, node.right)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "\n",
    "class XGBoostScratch:\n",
    "    def __init__(self,\n",
    "                 n_estimators=100,\n",
    "                 learning_rate=0.1,\n",
    "                 max_depth=3,\n",
    "                 min_samples_split=2):\n",
    "        self.n_estimators      = n_estimators\n",
    "        self.lr                = learning_rate\n",
    "        self.max_depth         = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees: List[DecisionTreeRegressorScratch] = []\n",
    "        self.init_score = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        # initialize log-odds\n",
    "        if hasattr(y, \"to_numpy\"):\n",
    "            y = y.to_numpy()\n",
    "\n",
    "        p = np.clip(y.mean(), 1e-5, 1 - 1e-5)\n",
    "        self.init_score = np.log(p / (1 - p))\n",
    "        y_pred = np.full_like(y, self.init_score, dtype=float)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # pseudo-residuals\n",
    "            pred_proba = 1 / (1 + np.exp(-y_pred))\n",
    "            grad = y - pred_proba\n",
    "\n",
    "            tree = DecisionTreeRegressorScratch(\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_depth=self.max_depth\n",
    "            )\n",
    "            tree.fit(X, grad)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            update = tree.predict(X)\n",
    "            y_pred += self.lr * update\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        y_pred = np.full(X.shape[0], self.init_score, dtype=float)\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.lr * tree.predict(X)\n",
    "        probs = 1 / (1 + np.exp(-y_pred))\n",
    "        return np.vstack([1 - probs, probs]).T\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class DraftPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        champion_info_path: str,\n",
    "        model_type: str = 'xgboost',\n",
    "        # shared hyper-params\n",
    "        lr: float = 0.01,\n",
    "        n_iter: int = 1000,\n",
    "        C: float = 1.0,\n",
    "        kernel: str = 'linear',\n",
    "        gamma: str = 'scale',\n",
    "        # RF / XGBoost\n",
    "        n_estimators: int = 100,\n",
    "        max_depth: int = None,\n",
    "        learning_rate: float = 0.1,\n",
    "        n_components: int = 100\n",
    "    ):\n",
    "        mt = model_type.lower()\n",
    "\n",
    "        if mt == 'logistic_regression':\n",
    "            # our scratch logistic takes C, lr, n_iter\n",
    "            self.model = LogisticRegressionScratch(\n",
    "                C=C,\n",
    "                lr=lr,\n",
    "                n_iter=n_iter\n",
    "            )\n",
    "\n",
    "        elif mt == 'svm':\n",
    "            # scratch SVM takes C, lr, n_iter, kernel, gamma\n",
    "            self.model = SVMScratch(\n",
    "                C=C,\n",
    "                lr=lr,\n",
    "                n_iter=n_iter,\n",
    "                kernel=kernel,\n",
    "                gamma=gamma,\n",
    "                n_components=n_components\n",
    "            )\n",
    "\n",
    "        elif mt == 'random_forest':\n",
    "            self.model = RandomForestScratch(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "\n",
    "        elif mt == 'xgboost':\n",
    "            self.model = XGBoostScratch(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_type '{model_type}'\")\n",
    "\n",
    "        # load champion info\n",
    "        with open(champion_info_path, 'r') as f:\n",
    "            data = json.load(f)[\"data\"]\n",
    "\n",
    "        self.champion_info = data\n",
    "        self.id_to_name = {\n",
    "            int(champ[\"id\"]): champ[\"key\"]\n",
    "            for champ in data.values()\n",
    "            if champ[\"key\"] != \"None\"\n",
    "        }\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        print(f\"Training {self.model.__class__.__name__} on {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.model.predict_proba(X)[:, 1]\n",
    "\n",
    "    def save(self, path: str):\n",
    "        joblib.dump({'model': self.model}, path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        state = joblib.load(path)\n",
    "        self.model = state['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from data_processor import DataProcessor\n",
    "from model import DraftPredictor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "def main():\n",
    "    # Parse command line arguments\n",
    "    # determine the directory this file lives in\n",
    "    HERE = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "    # build absolute defaults\n",
    "    DEFAULT_GAMES   = os.path.join(HERE, 'data', 'games.csv')\n",
    "    DEFAULT_CHAMPS  = os.path.join(HERE, 'data', 'champion_info.json')\n",
    "    DEFAULT_CHAMPS2  = os.path.join(HERE, 'data', 'champion_info_2.json')\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Train a League of Legends draft prediction model'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--games_path', \n",
    "        type=str,\n",
    "        default=DEFAULT_GAMES,\n",
    "        help='Path to the games data CSV file'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--champion_info_path',\n",
    "        type=str,\n",
    "        default=DEFAULT_CHAMPS,\n",
    "        help='Path to the champion info JSON file'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--champion_info_path_2',\n",
    "        type=str,\n",
    "        default=DEFAULT_CHAMPS2,\n",
    "        help='Path to the champion info 2 JSON file'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--test_size',\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help='Proportion of data to use for testing'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--random_state',\n",
    "        type=int,\n",
    "        default=42,\n",
    "        help='Random seed for reproducibility'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model_type',\n",
    "        type=str,\n",
    "        default='xgboost',\n",
    "        choices=['logistic_regression', 'svm', 'random_forest', 'xgboost'],\n",
    "        help='Which algorithm to use'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model_name',\n",
    "        type=str,\n",
    "        default='draft_predictor',\n",
    "        help='Name to save the model under'\n",
    "    )\n",
    "    \n",
    "    # RF & XGB\n",
    "    parser.add_argument('--n_estimators',   type=int,   nargs='+', default=[50,100])\n",
    "    parser.add_argument('--max_depth',      type=int,   nargs='+', default=[5,10])\n",
    "    parser.add_argument('--learning_rate',  type=float, nargs='+', default=[0.1,0.3])\n",
    "    # LR & SVM\n",
    "    parser.add_argument('--C',              type=float, nargs='+', default=[0.01,0.1,1,10])\n",
    "    parser.add_argument('--kernel',         type=str,   nargs='+', default=['linear','rbf'])\n",
    "    parser.add_argument('--gamma',          type=str,   nargs='+', default=['scale'])\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Create checkpoints directory if it doesn't exist\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "    # Initialize data processor and process data\n",
    "    print(\"Processing data...\")\n",
    "    data_processor = DataProcessor(args.games_path, args.champion_info_path, args.champion_info_path_2)\n",
    "    X_train, X_test, y_train, y_test = data_processor.prepare_train_test_split(\n",
    "        test_size=args.test_size,\n",
    "        random_state=args.random_state\n",
    "    )\n",
    "    print(\"Data processing complete.\")\n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    grid = []\n",
    "    if args.model_type in ('random_forest','xgboost'):\n",
    "        for n in args.n_estimators:\n",
    "            for d in args.max_depth:\n",
    "                for lr in (args.learning_rate if args.model_type=='xgboost' else [None]):\n",
    "                    params = {'n_estimators':n, 'max_depth':d}\n",
    "                    if lr is not None: params['learning_rate'] = lr\n",
    "                    grid.append(params)\n",
    "\n",
    "    elif args.model_type == 'logistic_regression':\n",
    "        for C in args.C:\n",
    "            grid.append({'C':C})\n",
    "\n",
    "    elif args.model_type == 'svm':\n",
    "        for C in args.C:\n",
    "            for kernel in args.kernel:\n",
    "                for gamma in args.gamma:\n",
    "                    grid.append({'C':C, 'kernel':kernel, 'gamma':gamma})\n",
    "                    \n",
    "                    \n",
    "    # 3) sweep\n",
    "    best_acc, best_params = 0.0, None\n",
    "    for params in grid:\n",
    "        print(\"→ trying\", args.model_type, params)\n",
    "        model = DraftPredictor(\n",
    "            model_type=args.model_type,\n",
    "            **params,champion_info_path=args.champion_info_path\n",
    "        )\n",
    "        model.train(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        print(\"   ↳ acc =\", acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc, best_params = acc, params.copy()\n",
    "\n",
    "    print(f\"Best validation accuracy = {best_acc:.4f} with {best_params}\")\n",
    "\n",
    "    # Initialize and train model\n",
    "    print(f\"Training a {args.model_type} model...\")\n",
    "    final_model = DraftPredictor(model_type=args.model_type, **best_params,champion_info_path=args.champion_info_path)\n",
    "    final_model.train(X_train, y_train)\n",
    "\n",
    "    # 5) evaluate on test set\n",
    "    y_pred      = final_model.predict(X_test)\n",
    "    print(\"Test set predictions:\", y_pred)\n",
    "    # for AUC we need scores / probabilities if available\n",
    "    try:\n",
    "        y_score = final_model.model.predict_proba(X_test)[:,1]\n",
    "    except AttributeError:\n",
    "        y_score = final_model.model.decision_function(X_test)\n",
    "    print(\"\\n=== TEST SET METRICS ===\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "    print(\"AUC-ROC  :\", roc_auc_score(y_test, y_score))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall   :\", recall_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "    # Save model and processor\n",
    "    model_path = os.path.join('checkpoints', f'{args.model_name}.joblib')\n",
    "    processor_path = os.path.join('checkpoints', f'{args.model_name}_processor.joblib')\n",
    "    \n",
    "    model.save(model_path)\n",
    "    data_processor.save(processor_path)\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    print(f\"Data processor saved to {processor_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Number of champions: 138\n",
      "Number of games: 48555\n",
      "Data processing complete.\n",
      "Training set size: 38844\n",
      "Test set size: 9711\n",
      "→ trying logistic_regression {'C': 0.01}\n",
      "Training LogisticRegressionScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5019050561219236\n",
      "→ trying logistic_regression {'C': 0.1}\n",
      "Training LogisticRegressionScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5191020492225311\n",
      "→ trying logistic_regression {'C': 1}\n",
      "Training LogisticRegressionScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5922150139017609\n",
      "→ trying logistic_regression {'C': 10}\n",
      "Training LogisticRegressionScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5938626300072083\n",
      "Best validation accuracy = 0.5939 with {'C': 10}\n",
      "Training a logistic_regression model...\n",
      "Training LogisticRegressionScratch on 38844 samples, 295 features\n",
      "Test set predictions: [1 0 1 ... 0 0 1]\n",
      "\n",
      "=== TEST SET METRICS ===\n",
      "Accuracy : 0.5938626300072083\n",
      "AUC-ROC  : 0.6269684704544176\n",
      "Precision: 0.5903614457831325\n",
      "Recall   : 0.62330734509643\n",
      "Model saved to checkpoints/draft_predictor.joblib\n",
      "Data processor saved to checkpoints/draft_predictor_processor.joblib\n"
     ]
    }
   ],
   "source": [
    "!python train_model.py --model_type logistic_regression  --games_path data/games.csv --champion_info_path data/champion_info.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Number of champions: 138\n",
      "Number of games: 48555\n",
      "Data processing complete.\n",
      "Training set size: 38844\n",
      "Test set size: 9711\n",
      "→ trying svm {'C': 0.01, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "Training SVMScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5019050561219236\n",
      "→ trying svm {'C': 0.01, 'kernel': 'rbf', 'gamma': 'scale'}\n",
      "Training SVMScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5019050561219236\n",
      "→ trying svm {'C': 0.1, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "Training SVMScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5019050561219236\n",
      "→ trying svm {'C': 0.1, 'kernel': 'rbf', 'gamma': 'scale'}\n",
      "Training SVMScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5019050561219236\n",
      "→ trying svm {'C': 1, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "Training SVMScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5024199361548759\n",
      "→ trying svm {'C': 1, 'kernel': 'rbf', 'gamma': 'scale'}\n",
      "Training SVMScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5019050561219236\n",
      "→ trying svm {'C': 10, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "Training SVMScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.500875296056019\n",
      "→ trying svm {'C': 10, 'kernel': 'rbf', 'gamma': 'scale'}\n",
      "Training SVMScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5019050561219236\n",
      "Best validation accuracy = 0.5024 with {'C': 1, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "Training a svm model...\n",
      "Training SVMScratch on 38844 samples, 295 features\n",
      "Test set predictions: [1 1 0 ... 0 0 1]\n",
      "\n",
      "=== TEST SET METRICS ===\n",
      "Accuracy : 0.5024199361548759\n",
      "AUC-ROC  : 0.49741638981897257\n",
      "Precision: 0.504585152838428\n",
      "Recall   : 0.4741485432909315\n",
      "Model saved to checkpoints/draft_predictor.joblib\n",
      "Data processor saved to checkpoints/draft_predictor_processor.joblib\n"
     ]
    }
   ],
   "source": [
    "!python train_model.py --model_type svm  --games_path data/games.csv --champion_info_path data/champion_info.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Number of champions: 138\n",
      "Number of games: 48555\n",
      "Data processing complete.\n",
      "Training set size: 38844\n",
      "Test set size: 9711\n",
      "→ trying random_forest {'n_estimators': 50, 'max_depth': 5}\n",
      "Training RandomForestScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5151889609720935\n",
      "→ trying random_forest {'n_estimators': 50, 'max_depth': 10}\n",
      "Training RandomForestScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.513953248893008\n",
      "→ trying random_forest {'n_estimators': 100, 'max_depth': 5}\n",
      "Training RandomForestScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5028318401812377\n",
      "→ trying random_forest {'n_estimators': 100, 'max_depth': 10}\n",
      "Training RandomForestScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5094223046030275\n",
      "Best validation accuracy = 0.5152 with {'n_estimators': 50, 'max_depth': 5}\n",
      "Training a random_forest model...\n",
      "Training RandomForestScratch on 38844 samples, 295 features\n",
      "Test set predictions: [1 1 1 ... 1 1 1]\n",
      "\n",
      "=== TEST SET METRICS ===\n",
      "Accuracy : 0.5060240963855421\n",
      "AUC-ROC  : 0.5891734687030262\n",
      "Precision: 0.5040091638029782\n",
      "Recall   : 0.9930242100943784\n",
      "Model saved to checkpoints/draft_predictor.joblib\n",
      "Data processor saved to checkpoints/draft_predictor_processor.joblib\n"
     ]
    }
   ],
   "source": [
    "!python train_model.py --model_type random_forest  --games_path data/games.csv --champion_info_path data/champion_info.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Number of champions: 138\n",
      "Number of games: 48555\n",
      "Data processing complete.\n",
      "Training set size: 38844\n",
      "Test set size: 9711\n",
      "→ trying xgboost {'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "Training XGBoostScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5923179899083514\n",
      "→ trying xgboost {'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.3}\n",
      "Training XGBoostScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5938626300072083\n",
      "→ trying xgboost {'n_estimators': 50, 'max_depth': 10, 'learning_rate': 0.1}\n",
      "Training XGBoostScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5862424055195139\n",
      "→ trying xgboost {'n_estimators': 50, 'max_depth': 10, 'learning_rate': 0.3}\n",
      "Training XGBoostScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5839769333745237\n",
      "→ trying xgboost {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "Training XGBoostScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5921120378951704\n",
      "→ trying xgboost {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.3}\n",
      "Training XGBoostScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5909793018226753\n",
      "→ trying xgboost {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.1}\n",
      "Training XGBoostScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5871691895788281\n",
      "→ trying xgboost {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.3}\n",
      "Training XGBoostScratch on 38844 samples, 295 features\n",
      "   ↳ acc = 0.5776953969725054\n",
      "Best validation accuracy = 0.5939 with {'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.3}\n",
      "Training a xgboost model...\n",
      "Training XGBoostScratch on 38844 samples, 295 features\n",
      "Test set predictions: [1 0 1 ... 0 0 1]\n",
      "\n",
      "=== TEST SET METRICS ===\n",
      "Accuracy : 0.5938626300072083\n",
      "AUC-ROC  : 0.6186753829329367\n",
      "Precision: 0.5903263403263403\n",
      "Recall   : 0.6235125153877719\n",
      "Model saved to checkpoints/draft_predictor.joblib\n",
      "Data processor saved to checkpoints/draft_predictor_processor.joblib\n"
     ]
    }
   ],
   "source": [
    "!python train_model.py --model_type xgboost  --games_path data/games.csv --champion_info_path data/champion_info.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def recommend_pick(\n",
    "    team1_champs: List[str],\n",
    "    team2_champs_partial: List[str],\n",
    "    processor,\n",
    "    model,\n",
    "    top_k: int = 5\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Recommend the final pick for team2 and return the win rate for team2.\n",
    "\n",
    "    Parameters:\n",
    "        team1_champs (List[str]): Full list of 5 champions for team1\n",
    "        team2_champs_partial (List[str]): First 4 champions for team2\n",
    "        processor: Trained DataProcessor\n",
    "        model: Trained DraftPredictor model\n",
    "        top_k (int): Number of recommendations to return\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: List of (champion_name, team2_win_probability), sorted descending\n",
    "    \"\"\"\n",
    "    all_champs = list(processor.champion_key_to_id.keys())\n",
    "\n",
    "    # Filter out already-picked champions\n",
    "    used = set(team1_champs + team2_champs_partial)\n",
    "    remaining = [c for c in all_champs if c not in used]\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    # Try each remaining champion as the 5th pick for team2\n",
    "    for champ in remaining:\n",
    "        team2_full = team2_champs_partial + [champ]\n",
    "\n",
    "        feats = processor.prepare_prediction_data(\n",
    "            team1_champs=team1_champs,\n",
    "            team2_champs=team2_full\n",
    "        )\n",
    "        \n",
    "        \n",
    "        prob_team1_win = model.predict_proba(feats).item()\n",
    "        prob_team2_win = 1.0 - prob_team1_win\n",
    "\n",
    "        recommendations.append((champ, prob_team2_win))\n",
    "\n",
    "    # Sort by team2 win probability (descending)\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return recommendations[:top_k]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from data_processor import DataProcessor\n",
    "    from model import DraftPredictor\n",
    "\n",
    "    PROC_PATH  = \"checkpoints/draft_predictor_processor.joblib\"\n",
    "    CHAMP_JSON = \"data/champion_info.json\"\n",
    "    MODEL_PATH = \"checkpoints/draft_predictor.joblib\"\n",
    "\n",
    "    processor = DataProcessor.load(PROC_PATH)\n",
    "    model = DraftPredictor(CHAMP_JSON,model_type='logistic_regression')\n",
    "    model.load(MODEL_PATH)\n",
    "\n",
    "    team1 = [\"Aatrox\", \"LeeSin\", \"Ahri\", \"Jinx\", \"Thresh\"]\n",
    "    team2_partial = [\"Ornn\", \"MasterYi\", \"Yasuo\", \"TahmKench\"]\n",
    "\n",
    "    top_recs = recommend_pick(team1, team2_partial, processor, model, top_k=8)\n",
    "\n",
    "    print(\"Top 8 recommended 5th picks for team2 (sorted by win probability):\\n\")\n",
    "    for i, (champ, prob) in enumerate(top_recs, 1):\n",
    "        print(f\"{i:>2}. {champ:<12}  →  team2 win rate: {prob:6.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
